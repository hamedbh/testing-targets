library(targets)
library(tarchetypes)
# Using a separate packages script seems to work fine, as it did under drake
source("packages.R")
purrr::walk(list.files("R", full.names = TRUE), 
            ~ source(.x))
options(tidymodels.dark = TRUE)
library(future)
plan(multisession)

tar_pipeline(
    # data wrangling steps
    tar_target(
        raw_data_file, 
        "data/diamonds.csv", 
        format = "file"
    ), 
    tar_target(
        raw_d,
        read_csv(raw_data_file),
        format = "fst_tbl"
    ),
    tar_target(
        clean_d,
        clean_diamonds(raw_d),
        format = "fst_tbl"
    ),
    # shared steps for modelling
    tar_target(
        gem_split,
        initial_split(clean_d, prop = 1/2, strata = cut)
    ),
    tar_target(
        gem_CV_folds,
        vfold_cv(training(gem_split), v = 5, strata = cut)
    ),
    tar_target(
        gem_rec,
        create_gem_rec(gem_split)
    ),
    # build an elastic net logistic regression model
    tar_target(
        gem_elnet_mod,
        logistic_reg(
            penalty = tune(),
            mixture = tune()
        ) %>%
            set_engine(
                "glmnet"
            )
    ),
    tar_target(
        gem_elnet_wfl,
        workflow() %>% add_recipe(gem_rec) %>% add_model(gem_elnet_mod)
    ),
    tar_target(
        gem_elnet_param,
        gem_elnet_wfl %>%
            parameters() %>% 
            update(mixture = mixture(range = c(0, 1)))
    ),
    tar_target(
        gem_elnet_grid,
        gem_elnet_param %>% grid_regular(levels = c(20, 11))
    ),
    tar_target(
        gem_elnet_tune,
        tune_grid(
            gem_elnet_wfl,
            resamples = gem_CV_folds,
            param_info = gem_elnet_param,
            grid = gem_elnet_grid,
            metrics = metric_set(roc_auc, pr_auc, gain_capture, mn_log_loss),
            control = control_grid(verbose = TRUE, save_pred = TRUE)
        )
    ),
    # build a MARS model
    tar_target(
        gem_mars_rec,
        create_mars_rec(gem_split)
    ),
    tar_target(
        gem_mars_mod,
        mars(
            mode = "classification",
            num_terms = tune(),
            prod_degree = 2
        ) %>%
            set_engine(
                "earth"
            )
    ),
    tar_target(
        gem_mars_wfl,
        workflow() %>% add_recipe(gem_mars_rec) %>% add_model(gem_mars_mod)
    ),
    tar_target(
        gem_mars_param,
        gem_mars_wfl %>%
            parameters() %>%
            update(
                num_terms = finalize(num_terms(),
                                     gem_mars_rec %>%
                                         prep() %>%
                                         juice())
            )
    ),
    tar_target(
        gem_mars_grid,
        gem_mars_param %>% grid_regular(levels = 4)
    ),
    tar_target(
        gem_mars_tune,
        tune_grid(
            gem_mars_wfl,
            resamples = gem_CV_folds,
            param_info = gem_mars_param,
            grid = gem_mars_grid,
            metrics = metric_set(roc_auc, pr_auc, gain_capture, mn_log_loss),
            control = control_grid(verbose = TRUE, save_pred = TRUE)
        )
    ), 
    # build an XGBoost model
    tar_target(
        gem_xgb_rec,
        create_xgb_rec(gem_split)
    ),
    tar_target(
        gem_xgb_mod,
        boost_tree(
            mode = "classification",
            mtry = tune(), 
            trees = 100L, 
            tree_depth = tune(), 
            learn_rate = 0.1, 
            sample_size = tune()
        ) %>%
            set_engine(
                "xgboost"
            )
    ),
    tar_target(
        gem_xgb_wfl,
        workflow() %>% add_recipe(gem_xgb_rec) %>% add_model(gem_xgb_mod)
    ),
    tar_target(
        gem_xgb_param,
        gem_xgb_wfl %>%
            parameters() %>%
            update(
                tree_depth = tree_depth(range = c(4, 10)), 
                mtry = mtry(range = 
                                c(
                                    round((gem_xgb_rec %>% 
                                               prep() %>% 
                                               juice() %>% 
                                               ncol()) * 0.4), 
                                    (gem_xgb_rec %>% 
                                         prep() %>% 
                                         juice() %>% 
                                         ncol) - 1L
                                )), 
                sample_size = sample_prop(range = c(0.5, 1))
            )
    ),
    tar_target(
        gem_xgb_grid,
        gem_xgb_param %>% grid_regular(levels = c(4, 4, 3))
    ),
    tar_target(
        gem_xgb_tune,
        tune_grid(
            gem_xgb_wfl,
            resamples = gem_CV_folds,
            param_info = gem_xgb_param,
            grid = gem_xgb_grid,
            metrics = metric_set(roc_auc, pr_auc, gain_capture, mn_log_loss),
            control = control_grid(verbose = TRUE, save_pred = TRUE)
        )
    ),
    # pull out the best parameters for each model type, fit a model on the 
    # training data, and collect predictions made with those parameters
    tar_target(
        gem_elnet_best_params, 
        gem_elnet_tune %>%
            select_by_pct_loss(desc(penalty),
                               metric = "roc_auc",
                               limit = 1)
    ), 
    tar_target(
        gem_mars_best_params, 
        gem_mars_tune %>%
            select_by_pct_loss(num_terms,
                               metric = "roc_auc",
                               limit = 1)
    ), 
    tar_target(
        gem_xgb_best_params, 
        gem_xgb_tune %>% 
            select_by_pct_loss(tree_depth, sample_size, mtry, 
                               metric = "roc_auc", 
                               limit = 1)
    ), 
    tar_target(
        gem_elnet_fit, 
        gem_elnet_wfl %>%
            finalize_workflow(gem_elnet_best_params) %>%
            fit(data = training(gem_split))
    ), 
    tar_target(
        gem_mars_fit, 
        gem_mars_wfl %>%
            finalize_workflow(gem_mars_best_params) %>%
            fit(data = training(gem_split))
    ), 
    tar_target(
        gem_xgb_fit, 
        gem_xgb_wfl %>%
            finalize_workflow(gem_xgb_best_params) %>%
            fit(data = training(gem_split))
    ), 
    tar_target(
        gem_elnet_preds, 
        gem_elnet_tune %>% 
            collect_predictions() %>% 
            semi_join(gem_elnet_best_params, 
                      by = c("penalty", "mixture"))
    ), 
    tar_target(
        gem_elnet_platt, 
        gem_elnet_preds %>% 
            calibrate_prob_model(.pred_ideal, cut, method = "platt")
    ), 
    tar_target(
        gem_elnet_iso, 
        gem_elnet_preds %>% 
            calibrate_prob_model(.pred_ideal, cut, method = "isotonic")
    ), 
    tar_target(
        gem_mars_preds, 
        gem_mars_tune %>% 
            collect_predictions() %>% 
            semi_join(gem_mars_best_params, 
                      by = c("num_terms"))
    ), 
    tar_target(
        gem_mars_platt, 
        gem_mars_preds %>% 
            calibrate_prob_model(.pred_ideal, cut, method = "platt")
    ), 
    tar_target(
        gem_mars_iso, 
        gem_mars_preds %>% 
            calibrate_prob_model(.pred_ideal, cut, method = "isotonic")
    ), 
    tar_target(
        gem_xgb_preds, 
        gem_xgb_tune %>% 
            collect_predictions() %>% 
            semi_join(gem_xgb_best_params, 
                      by = c("mtry", "sample_size", "tree_depth"))
    ), 
    tar_target(
        gem_xgb_platt, 
        gem_xgb_preds %>% 
            calibrate_prob_model(.pred_ideal, cut, method = "platt")
    ), 
    tar_target(
        gem_xgb_iso, 
        gem_xgb_preds %>% 
            calibrate_prob_model(.pred_ideal, cut, method = "isotonic")
    )
)
